{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from transformers import Transformer\n",
    "from transformer_train import Batch, NoamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(input_vocab=10, output_vocab=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using the transformer with None mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.3559, -5.2487, -0.7497,  ..., -5.3695, -1.3660, -3.9769],\n",
       "         [-5.2369, -4.1565, -0.7808,  ..., -3.6044, -2.5541, -4.1441],\n",
       "         [-3.7091, -3.8110, -1.6015,  ..., -3.8261, -1.2818, -4.3744],\n",
       "         ...,\n",
       "         [-4.5364, -5.4882, -0.9958,  ..., -4.4025, -2.8058, -4.4474],\n",
       "         [-2.2326, -3.7237, -0.6873,  ..., -3.8603, -2.7467, -4.3089],\n",
       "         [-2.2533, -3.8377, -0.9976,  ..., -4.6721, -1.2523, -4.1875]],\n",
       "\n",
       "        [[-4.1219, -3.8285, -2.2555,  ..., -1.3040, -2.0745, -4.4725],\n",
       "         [-6.8820, -4.5870, -1.6347,  ..., -2.0513, -3.3394, -4.2462],\n",
       "         [-4.2103, -3.5013, -2.2617,  ..., -1.4769, -1.9338, -4.4816],\n",
       "         ...,\n",
       "         [-5.5928, -4.4443, -2.1528,  ..., -2.3184, -2.2187, -4.1659],\n",
       "         [-6.3742, -4.5327, -2.5778,  ..., -1.4635, -2.0110, -4.7276],\n",
       "         [-6.3213, -4.5722, -2.4557,  ..., -1.4363, -2.0143, -4.7555]],\n",
       "\n",
       "        [[-3.9285, -4.4695, -1.3114,  ..., -2.3223, -2.6220, -1.6668],\n",
       "         [-2.4939, -3.0806, -2.1883,  ..., -2.9426, -1.3752, -1.9386],\n",
       "         [-1.5447, -4.0258, -1.4670,  ..., -3.2358, -1.7055, -2.4418],\n",
       "         ...,\n",
       "         [-4.3501, -4.6258, -1.0932,  ..., -2.0549, -2.9518, -1.7605],\n",
       "         [-2.2363, -3.1196, -0.9678,  ..., -2.0272, -2.9244, -2.6154],\n",
       "         [-3.9947, -3.6005, -0.7453,  ..., -2.1403, -2.1210, -2.4593]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.2128, -1.8378, -1.6126,  ..., -3.2480, -2.0294, -4.4794],\n",
       "         [-3.4544, -2.0358, -2.2145,  ..., -2.5965, -1.6908, -4.1507],\n",
       "         [-3.4072, -3.1864, -1.6403,  ..., -2.0351, -2.8860, -3.4985],\n",
       "         ...,\n",
       "         [-1.5498, -1.8377, -1.2133,  ..., -2.8599, -2.3108, -4.5826],\n",
       "         [-1.5051, -2.0085, -1.1467,  ..., -2.9125, -2.3151, -4.5541],\n",
       "         [-1.4617, -2.1619, -1.0864,  ..., -2.9017, -2.3686, -4.6215]],\n",
       "\n",
       "        [[-3.2055, -2.6002, -1.7654,  ..., -0.9891, -2.3209, -2.8478],\n",
       "         [-2.8025, -3.1932, -1.1513,  ..., -2.6364, -1.2044, -3.0948],\n",
       "         [-3.4757, -3.2465, -1.6979,  ..., -1.3024, -1.5692, -3.7252],\n",
       "         ...,\n",
       "         [-6.2538, -3.8278, -0.9712,  ..., -1.6918, -3.4517, -3.4258],\n",
       "         [-5.0230, -3.8922, -1.4335,  ..., -1.9344, -1.8840, -2.9970],\n",
       "         [-6.1582, -4.0273, -0.8903,  ..., -1.7405, -3.5611, -3.4634]],\n",
       "\n",
       "        [[-5.5136, -3.6622, -2.5887,  ..., -2.8017, -0.5405, -5.3403],\n",
       "         [-1.2616, -3.8038, -1.9165,  ..., -3.0514, -1.1259, -5.3920],\n",
       "         [-3.5493, -4.6502, -1.2800,  ..., -3.3501, -1.1348, -4.7484],\n",
       "         ...,\n",
       "         [-4.2713, -5.1331, -1.3458,  ..., -2.3367, -2.6419, -4.6127],\n",
       "         [-1.4754, -3.8504, -1.3018,  ..., -2.8413, -1.4184, -5.4261],\n",
       "         [-3.7437, -5.1079, -0.8129,  ..., -3.4816, -1.5084, -4.8721]]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randint(10, (60,5))\n",
    "output = torch.randint(10, (60,10))\n",
    "transformer(input, output, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using the transformer with real mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.3160, -3.0536, -1.9481,  ..., -1.7658, -3.3523, -1.4735],\n",
       "         [-3.1745, -3.1472, -2.1283,  ..., -1.4101, -4.1834, -1.0412],\n",
       "         [-3.3421, -3.7672, -1.9132,  ..., -1.8864, -4.0480, -0.7068],\n",
       "         ...,\n",
       "         [-5.2904, -3.5835, -1.4576,  ..., -2.8472, -4.2545, -1.5367],\n",
       "         [-7.2072, -3.5299, -1.8865,  ..., -1.4819, -3.2552, -1.6378],\n",
       "         [-3.1346, -3.7828, -0.9886,  ..., -3.2449, -3.2661, -1.0610]],\n",
       "\n",
       "        [[-4.7001, -6.8197, -1.6232,  ..., -4.6834, -1.3834, -5.2095],\n",
       "         [-2.7843, -4.9023, -1.4748,  ..., -3.3052, -1.9544, -4.9664],\n",
       "         [-3.1717, -5.1768, -2.4111,  ..., -4.1504, -0.8286, -4.9793],\n",
       "         ...,\n",
       "         [-6.6173, -6.1197, -0.8461,  ..., -3.7754, -2.9967, -4.7263],\n",
       "         [-3.9399, -5.9752, -1.5434,  ..., -3.3832, -1.5053, -4.6520],\n",
       "         [-5.2750, -5.6847, -0.7923,  ..., -3.9814, -2.6309, -4.5875]],\n",
       "\n",
       "        [[-3.8285, -5.2046, -1.5694,  ..., -2.7178, -3.5082, -2.8018],\n",
       "         [-4.0688, -5.0071, -1.9951,  ..., -1.9424, -2.1109, -3.0420],\n",
       "         [-1.2300, -5.2289, -1.3132,  ..., -2.9336, -2.5851, -3.1433],\n",
       "         ...,\n",
       "         [-4.1613, -5.4556, -1.7614,  ..., -1.9931, -4.4816, -3.2572],\n",
       "         [-3.6580, -6.7654, -0.9240,  ..., -3.0934, -3.3716, -3.6072],\n",
       "         [-4.9930, -5.6708, -1.5481,  ..., -1.7254, -2.9613, -3.4398]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.8472, -3.3891, -0.5993,  ..., -2.6107, -2.1093, -4.7353],\n",
       "         [-2.7700, -3.3761, -1.7782,  ..., -1.7637, -1.2010, -4.7649],\n",
       "         [-2.1659, -2.8181, -0.9126,  ..., -2.2969, -2.0454, -4.5792],\n",
       "         ...,\n",
       "         [-6.0138, -4.2227, -2.3902,  ..., -2.3916, -0.7118, -3.7378],\n",
       "         [-5.5169, -4.0634, -0.5628,  ..., -2.4577, -3.0492, -4.4306],\n",
       "         [-3.9517, -4.6710, -0.8111,  ..., -2.5645, -2.7956, -3.2843]],\n",
       "\n",
       "        [[-3.9993, -3.3061, -1.2710,  ..., -1.1343, -2.3702, -3.9435],\n",
       "         [-2.2367, -2.8334, -2.4946,  ..., -1.7709, -1.2990, -3.7137],\n",
       "         [-4.8033, -3.9214, -2.0728,  ..., -1.7216, -1.7843, -3.6091],\n",
       "         ...,\n",
       "         [-2.9121, -3.9054, -2.0506,  ..., -1.1272, -2.6515, -3.6084],\n",
       "         [-4.4403, -4.8306, -1.6753,  ..., -1.5353, -3.4527, -2.8842],\n",
       "         [-4.1871, -4.6726, -1.3419,  ..., -1.2335, -3.5404, -3.4455]],\n",
       "\n",
       "        [[-2.5047, -2.2298, -2.4832,  ..., -2.6418, -0.7256, -4.9040],\n",
       "         [-5.9068, -2.1591, -2.1442,  ..., -1.0896, -2.5014, -4.7322],\n",
       "         [-2.9141, -2.0081, -3.2267,  ..., -1.1808, -1.2244, -5.2661],\n",
       "         ...,\n",
       "         [-3.3608, -1.6251, -2.5191,  ..., -1.1389, -1.7539, -5.2125],\n",
       "         [-6.0550, -2.8157, -3.6087,  ..., -1.5688, -0.9520, -5.0247],\n",
       "         [-3.4302, -2.3716, -2.2967,  ..., -1.0879, -2.0160, -5.1733]]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randint(10, (60,5))\n",
    "output = torch.randint(10, (60,10))\n",
    "input_mask = torch.randint(2, (60, 1, 5))\n",
    "output_mask = torch.randint(2, (60, 10, 10))\n",
    "transformer(input, output, input_mask, output_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training transformer for copy task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vocab = 11\n",
    "seq_len = 10\n",
    "model_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_data_generator(num_batches=30, batch_size=20):\n",
    "    for _ in range(num_batches):\n",
    "        data = np.random.randint(num_vocab, size=(batch_size, seq_len))\n",
    "        src = torch.from_numpy(data)\n",
    "        tgt = torch.from_numpy(data.copy())\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(input_vocab=num_vocab, output_vocab=num_vocab, model_dim=model_dim, num_coder=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "noam_optimizer = NoamOptimizer(adam_optimizer, model_dim, 1, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.3245237350463865\n",
      "1 1.82636163632075\n",
      "2 1.6100578824679057\n",
      "3 1.2480957667032877\n",
      "4 0.9334281404813131\n",
      "5 0.818869560956955\n",
      "6 0.8302984317143758\n",
      "7 0.7845571974913279\n",
      "8 0.7216464360555013\n",
      "9 0.7322157025337219\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    transformer.train()\n",
    "    data_iter = copy_data_generator()\n",
    "\n",
    "    losses = 0.\n",
    "    cnt = 0\n",
    "\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = transformer(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "\n",
    "        out = out.reshape(-1, out.shape[-1])\n",
    "        labels = batch.trg_y.reshape(-1)\n",
    "\n",
    "        loss = loss_fn(out, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        noam_optimizer.step()\n",
    "        noam_optimizer.optimizer.zero_grad()\n",
    "\n",
    "        losses += loss.item()\n",
    "        cnt += 1\n",
    "\n",
    "    print(epoch, losses / cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing transformer for copy task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.randint(num_vocab, size=(10, seq_len))\n",
    "\n",
    "expected_output = torch.from_numpy(input.copy()) \n",
    "output_init = torch.from_numpy(input[:,0].reshape(-1,1))\n",
    "input = torch.from_numpy(input)\n",
    "input_mask = torch.ones(10, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer.greedy_decode(input, output_init, seq_len, input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  3, 10,  2,  1,  7,  7,  1,  9,  9],\n",
       "        [ 4,  0,  1,  0,  5,  0,  9,  0, 10,  1],\n",
       "        [ 9, 10,  8,  0,  1,  1,  2,  3,  4,  3],\n",
       "        [10,  8,  9,  2,  7,  0, 10, 10,  7, 10],\n",
       "        [ 1,  7,  2,  3,  7,  8,  5,  1,  7,  9],\n",
       "        [ 1,  6, 10,  3,  4,  6,  7,  0,  9,  1],\n",
       "        [ 6,  7,  8,  4,  5,  0,  7,  2,  1,  1],\n",
       "        [ 2, 10, 10,  3,  0,  1,  6,  3, 10,  5],\n",
       "        [ 0,  5,  2,  6, 10,  6,  6,  1,  3,  7],\n",
       "        [ 8, 10,  6,  1,  4,  2,  9,  3,  1,  7]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  3, 10,  2,  1,  7,  1,  7,  9,  9],\n",
       "        [ 4,  3,  1,  1,  5,  6,  9,  3, 10,  1],\n",
       "        [ 9, 10,  8,  8,  1,  8,  2,  3,  4,  3],\n",
       "        [10,  8,  9,  2,  7,  2, 10, 10,  7, 10],\n",
       "        [ 1,  7,  2,  3,  7,  8,  5,  1,  7,  9],\n",
       "        [ 1,  6, 10,  3,  4,  6,  7,  7,  9,  1],\n",
       "        [ 6,  7,  8,  4,  5,  5,  7,  2,  1,  6],\n",
       "        [ 2, 10, 10,  3, 10,  3,  6,  3, 10,  5],\n",
       "        [ 0,  5,  2,  6, 10,  6,  6,  1,  3,  7],\n",
       "        [ 8, 10,  6,  1,  4,  2,  9,  3,  3,  7]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500000238418579"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get accuracy \n",
    "acc = (output == expected_output).sum() / (10 * seq_len)\n",
    "acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training transformer for sequence reversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vocab = 11\n",
    "seq_len = 10\n",
    "model_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_data_generator(num_batches=30, batch_size=20):\n",
    "    for _ in range(num_batches):\n",
    "        data = np.random.randint(num_vocab, size=(batch_size, seq_len))\n",
    "        src = torch.from_numpy(data)\n",
    "        tgt = torch.from_numpy(data[:, ::-1].copy())\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(input_vocab=num_vocab, output_vocab=num_vocab, model_dim=model_dim, num_coder=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "noam_optimizer = NoamOptimizer(adam_optimizer, model_dim, 1, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.3368816216786703\n",
      "1 1.8293853362401327\n",
      "2 1.7079774181048075\n",
      "3 1.3476770341396331\n",
      "4 0.9670732994874318\n",
      "5 0.8006935338179271\n",
      "6 0.7373670717080434\n",
      "7 0.7913566052913665\n",
      "8 0.8067436377207439\n",
      "9 0.8073766867319743\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    transformer.train()\n",
    "    data_iter = reverse_data_generator()\n",
    "\n",
    "    losses = 0.\n",
    "    cnt = 0\n",
    "\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = transformer(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "\n",
    "        out = out.reshape(-1, out.shape[-1])\n",
    "        labels = batch.trg_y.reshape(-1)\n",
    "\n",
    "        loss = loss_fn(out, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        noam_optimizer.step()\n",
    "        noam_optimizer.optimizer.zero_grad()\n",
    "\n",
    "        losses += loss.item()\n",
    "        cnt += 1\n",
    "\n",
    "    print(epoch, losses / cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing transformer for sequence reversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.randint(num_vocab, size=(10, seq_len))\n",
    "\n",
    "expected_output = torch.from_numpy(input[:, ::-1].copy()) \n",
    "output_init = torch.from_numpy(input[:,-1].reshape(-1,1))\n",
    "input = torch.from_numpy(input)\n",
    "input_mask = torch.ones(10, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer.greedy_decode(input, output_init, seq_len, input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9,  0,  8,  7,  7,  7, 10,  8,  3,  8],\n",
       "        [ 0,  5,  5,  8,  6,  8,  8,  8,  6,  7],\n",
       "        [ 5,  9,  9,  9,  0,  3,  0,  0,  0,  5],\n",
       "        [ 0,  8,  2,  0,  9,  5,  1,  1,  0,  2],\n",
       "        [ 9,  5,  1,  4,  1,  7,  1,  6,  3,  1],\n",
       "        [ 3,  7,  0, 10,  1, 10,  9,  9,  6,  4],\n",
       "        [ 3,  0,  6,  2,  3,  4,  1,  9,  4, 10],\n",
       "        [ 2,  0,  0,  6,  7,  2,  1,  4,  4,  2],\n",
       "        [ 0, 10,  4,  0,  1,  8, 10,  3,  2,  6],\n",
       "        [ 9,  8,  1,  8,  8,  5,  6,  7,  9,  0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9,  8,  7,  8,  7,  7, 10,  8,  3,  8],\n",
       "        [ 0,  5,  5,  8,  6,  8,  8,  8,  6,  7],\n",
       "        [ 5,  9,  9,  9,  9,  3,  9,  7,  5,  5],\n",
       "        [ 0,  8,  2,  9,  8,  9,  1,  1,  9,  5],\n",
       "        [ 9,  5,  1,  1,  4,  7,  1,  6,  3,  1],\n",
       "        [ 3,  7,  8, 10,  1, 10,  9,  9,  6,  4],\n",
       "        [ 3,  6,  6,  2,  3,  4,  1,  9,  4, 10],\n",
       "        [ 2,  8,  7,  6,  7,  2,  1,  4,  4,  2],\n",
       "        [ 0, 10, 10,  4,  1,  8, 10,  3,  3,  2],\n",
       "        [ 9,  8,  1,  8,  8,  5,  6,  7,  9,  9]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7699999809265137"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get accuracy \n",
    "acc = (output == expected_output).sum() / (10 * seq_len)\n",
    "acc.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
