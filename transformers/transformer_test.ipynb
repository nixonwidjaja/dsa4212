{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from transformers import Transformer\n",
    "from transformer_train import Batch, NoamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(input_vocab=10, output_vocab=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using the transformer with None mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.7241, -1.6738, -1.7825,  ..., -2.9035, -2.7298, -2.9162],\n",
       "         [-6.3393, -3.1287, -1.2187,  ..., -2.8651, -3.8741, -1.5978],\n",
       "         [-4.8164, -3.8533, -2.7677,  ..., -0.8371, -4.2480, -2.8280],\n",
       "         ...,\n",
       "         [-6.1315, -3.0389, -1.4388,  ..., -3.3017, -3.7573, -1.2052],\n",
       "         [-7.3743, -1.7026, -1.8243,  ..., -3.2567, -2.4396, -2.4312],\n",
       "         [-4.6603, -4.1607, -2.3987,  ..., -1.8191, -4.1767, -2.0908]],\n",
       "\n",
       "        [[-5.2516, -2.9991, -1.8385,  ..., -1.4333, -3.4890, -1.8554],\n",
       "         [-6.9604, -1.7433, -1.2655,  ..., -2.5555, -2.1312, -3.8134],\n",
       "         [-5.7765, -3.1944, -0.8544,  ..., -3.0560, -3.6236, -1.5050],\n",
       "         ...,\n",
       "         [-6.7821, -1.5662, -1.4108,  ..., -3.0513, -1.9618, -3.5587],\n",
       "         [-5.5438, -4.5060, -1.9625,  ..., -2.8324, -2.9298, -1.4453],\n",
       "         [-4.1153, -3.5120, -2.3348,  ..., -1.4389, -3.8671, -2.1483]],\n",
       "\n",
       "        [[-5.3493, -4.4682, -1.0783,  ..., -2.1520, -3.8770, -2.9099],\n",
       "         [-5.3334, -4.5377, -1.0115,  ..., -2.2256, -3.8300, -2.9414],\n",
       "         [-5.2955, -4.5746, -0.9698,  ..., -2.3169, -3.7642, -2.9995],\n",
       "         ...,\n",
       "         [-6.0910, -4.2569, -1.1149,  ..., -3.5445, -2.5471, -1.6877],\n",
       "         [-5.4792, -5.1301, -1.1262,  ..., -3.3583, -3.7600, -2.1463],\n",
       "         [-6.6985, -5.2070, -1.6442,  ..., -3.4600, -2.4094, -2.6053]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.9501, -3.4075, -1.7821,  ..., -4.5716, -5.0834, -0.4002],\n",
       "         [-5.4378, -5.2320, -1.9995,  ..., -3.9322, -3.9799, -0.3778],\n",
       "         [-4.0174, -3.5941, -2.2114,  ..., -3.1855, -3.3926, -0.7402],\n",
       "         ...,\n",
       "         [-5.3252, -5.1802, -1.7635,  ..., -4.9353, -5.5393, -0.3899],\n",
       "         [-3.5196, -3.7073, -1.8080,  ..., -3.8036, -4.5703, -0.5575],\n",
       "         [-5.2185, -5.5353, -2.2184,  ..., -4.0285, -4.1965, -0.2980]],\n",
       "\n",
       "        [[-6.0405, -5.0004, -2.7822,  ..., -3.6833, -3.7347, -0.4591],\n",
       "         [-5.6327, -3.7123, -2.9614,  ..., -2.9942, -2.9184, -0.6032],\n",
       "         [-6.1104, -4.0511, -2.0740,  ..., -4.4272, -4.2699, -0.2969],\n",
       "         ...,\n",
       "         [-5.8850, -5.1650, -2.8429,  ..., -3.9825, -3.7695, -0.3951],\n",
       "         [-5.1964, -4.1482, -2.6703,  ..., -3.0081, -4.1286, -0.3302],\n",
       "         [-4.5459, -4.1151, -2.7536,  ..., -3.6802, -4.7234, -0.4389]],\n",
       "\n",
       "        [[-6.7061, -2.3954, -1.5126,  ..., -1.5329, -4.6565, -1.6164],\n",
       "         [-4.6462, -3.1859, -0.7130,  ..., -1.9662, -5.1825, -2.1843],\n",
       "         [-5.6950, -2.9062, -1.2740,  ..., -2.2092, -4.4560, -1.0980],\n",
       "         ...,\n",
       "         [-7.4268, -1.2393, -0.8398,  ..., -4.2423, -3.4199, -2.3585],\n",
       "         [-6.3624, -3.7539, -1.1633,  ..., -4.0547, -6.1656, -0.8174],\n",
       "         [-5.5641, -3.3026, -1.4097,  ..., -2.3147, -4.5844, -0.8401]]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randint(10, (60,5))\n",
    "output = torch.randint(10, (60,10))\n",
    "transformer(input, output, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using the transformer with real mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1784, -2.4380, -2.8516,  ..., -4.0840, -2.8811, -0.7360],\n",
       "         [-1.9744, -3.6604, -3.3502,  ..., -4.2985, -3.4730, -0.7521],\n",
       "         [-3.0202, -2.5104, -4.3819,  ..., -3.2995, -2.4610, -0.8855],\n",
       "         ...,\n",
       "         [-2.3645, -3.5712, -3.7282,  ..., -4.3372, -3.1454, -0.6048],\n",
       "         [-1.4906, -3.3852, -3.6559,  ..., -4.3083, -3.0591, -1.2246],\n",
       "         [-1.9915, -2.1084, -4.4619,  ..., -4.5640, -3.3588, -0.6902]],\n",
       "\n",
       "        [[-4.9137, -3.9449, -3.0051,  ..., -2.7996, -3.9877, -2.0667],\n",
       "         [-5.9068, -5.1644, -3.5201,  ..., -2.7560, -2.9781, -2.9157],\n",
       "         [-4.4664, -4.0871, -2.9414,  ..., -2.9199, -4.0902, -1.9247],\n",
       "         ...,\n",
       "         [-5.3064, -3.7297, -2.2830,  ..., -2.4822, -2.1513, -2.8990],\n",
       "         [-4.5457, -7.2209, -3.0816,  ..., -3.5740, -5.1189, -2.0954],\n",
       "         [-5.9209, -5.2169, -3.8749,  ..., -1.6439, -4.0117, -2.6541]],\n",
       "\n",
       "        [[-6.5225, -3.5297, -3.5759,  ..., -1.4199, -3.2896, -2.6221],\n",
       "         [-6.0816, -4.6127, -3.5266,  ..., -2.6834, -2.5993, -2.8059],\n",
       "         [-6.8296, -2.6718, -2.5656,  ..., -3.3304, -1.9086, -3.2301],\n",
       "         ...,\n",
       "         [-3.7549, -4.5637, -4.0490,  ..., -3.2318, -4.6625, -2.6503],\n",
       "         [-6.0571, -4.1677, -2.3379,  ..., -2.6018, -2.9599, -0.7791],\n",
       "         [-5.4651, -4.7666, -2.5458,  ..., -2.6121, -3.0948, -1.7088]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.5608, -5.0439, -2.6163,  ..., -3.4787, -3.3135, -1.7869],\n",
       "         [-2.8444, -3.7551, -2.7760,  ..., -3.3639, -3.3120, -2.8450],\n",
       "         [-4.6162, -3.8139, -1.5089,  ..., -3.4970, -1.5393, -2.1388],\n",
       "         ...,\n",
       "         [-4.2159, -3.6717, -3.5727,  ..., -3.7345, -1.4330, -2.5786],\n",
       "         [-5.2556, -2.8498, -2.2409,  ..., -3.9682, -2.4173, -1.5857],\n",
       "         [-4.5516, -2.9700, -2.3008,  ..., -3.7677, -2.0219, -1.6669]],\n",
       "\n",
       "        [[-6.2773, -2.6992, -1.3792,  ..., -2.2467, -3.2901, -3.0243],\n",
       "         [-7.1242, -4.1058, -2.5093,  ..., -1.0043, -4.8848, -2.5576],\n",
       "         [-5.6271, -4.9725, -2.7238,  ..., -1.6767, -4.4212, -2.9899],\n",
       "         ...,\n",
       "         [-4.1650, -4.4699, -2.2690,  ..., -2.2179, -5.7024, -2.5348],\n",
       "         [-3.9122, -3.8157, -2.5597,  ..., -1.2239, -5.1157, -1.6679],\n",
       "         [-5.0975, -4.0570, -2.0266,  ..., -1.2351, -4.4802, -1.7981]],\n",
       "\n",
       "        [[-6.0789, -3.0934, -2.5488,  ..., -2.7483, -2.1994, -3.4575],\n",
       "         [-5.2305, -5.7894, -2.6528,  ..., -2.3021, -3.9358, -2.6560],\n",
       "         [-4.5574, -3.9488, -1.3514,  ..., -2.7984, -3.9589, -1.7347],\n",
       "         ...,\n",
       "         [-3.8768, -4.9246, -3.1410,  ..., -2.1397, -4.6109, -2.9468],\n",
       "         [-5.3640, -5.8563, -2.7765,  ..., -2.2472, -4.0159, -2.3615],\n",
       "         [-4.9195, -5.5591, -2.6311,  ..., -2.1666, -3.7166, -2.2272]]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randint(10, (60,5))\n",
    "output = torch.randint(10, (60,10))\n",
    "input_mask = torch.randint(2, (60, 1, 5))\n",
    "output_mask = torch.randint(2, (60, 10, 10))\n",
    "transformer(input, output, input_mask, output_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training transformer for copy task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vocab = 11\n",
    "seq_len = 10\n",
    "model_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_data_generator(num_batches=30, batch_size=20):\n",
    "    for _ in range(num_batches):\n",
    "        data = np.random.randint(num_vocab, size=(batch_size, seq_len))\n",
    "        src = torch.from_numpy(data)\n",
    "        tgt = torch.from_numpy(data.copy())\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(input_vocab=num_vocab, output_vocab=num_vocab, model_dim=model_dim, num_coder=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "noam_optimizer = NoamOptimizer(adam_optimizer, model_dim, 1, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.2781896273295086\n",
      "1 1.811338988939921\n",
      "2 1.6485260645548503\n",
      "3 1.2655906836191813\n",
      "4 1.0189296901226044\n",
      "5 0.8298549135526021\n",
      "6 0.7520816405614217\n",
      "7 0.7098671356836955\n",
      "8 0.8333705087502797\n",
      "9 0.8598594109217326\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    transformer.train()\n",
    "    data_iter = copy_data_generator()\n",
    "\n",
    "    losses = 0.\n",
    "    cnt = 0\n",
    "\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = transformer(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "\n",
    "        out = out.reshape(-1, out.shape[-1])\n",
    "        labels = batch.trg_y.reshape(-1)\n",
    "\n",
    "        loss = loss_fn(out, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        noam_optimizer.step()\n",
    "        noam_optimizer.optimizer.zero_grad()\n",
    "\n",
    "        losses += loss.item()\n",
    "        cnt += 1\n",
    "\n",
    "    print(epoch, losses / cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing transformer for copy task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.randint(num_vocab, size=(10, seq_len))\n",
    "\n",
    "expected_output = torch.from_numpy(input.copy()) \n",
    "output_init = torch.from_numpy(input[:,0].reshape(-1,1))\n",
    "input = torch.from_numpy(input)\n",
    "input_mask = torch.ones(10, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer.greedy_decode(input, output_init, seq_len, input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  2,  9,  4,  3,  6,  3,  1,  1],\n",
       "        [ 4,  2,  2,  2,  6, 10,  6,  7,  5,  9],\n",
       "        [10,  7,  0,  3,  5,  8,  5,  7,  9,  3],\n",
       "        [ 7,  9,  6,  8,  7,  8, 10,  5,  8,  6],\n",
       "        [ 5, 10,  3, 10,  1,  9,  7,  4,  9,  4],\n",
       "        [ 1,  0,  3,  7,  6,  9,  3,  6,  2,  3],\n",
       "        [ 3, 10,  9,  6,  9,  2, 10,  6,  5,  2],\n",
       "        [ 9,  3,  1,  2,  5,  4,  4,  2,  9,  3],\n",
       "        [ 0,  3,  1,  0,  7,  5,  2,  6,  2,  6],\n",
       "        [ 4,  0,  5,  9,  6,  8, 10,  2,  7,  0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  2,  9,  4,  3,  6,  1,  3,  1],\n",
       "        [ 4,  2,  6,  2,  6, 10,  6,  7,  5,  9],\n",
       "        [10,  7,  6,  3,  5,  8,  5,  7,  9,  3],\n",
       "        [ 7,  9,  6,  7,  8, 10,  8,  5,  8,  6],\n",
       "        [ 5, 10,  3, 10,  1,  9,  7,  4,  9,  4],\n",
       "        [ 1,  3,  7,  6,  6,  9,  3,  6,  3,  2],\n",
       "        [ 3, 10,  9,  6,  9, 10,  2,  6,  5,  2],\n",
       "        [ 9,  3,  1,  5,  4,  4,  4,  2,  9,  3],\n",
       "        [ 0,  3,  1,  6,  7,  5,  2,  6,  2,  6],\n",
       "        [ 4,  7,  5,  9,  6,  8, 10,  2,  7, 10]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.800000011920929"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get accuracy \n",
    "acc = (output == expected_output).sum() / (10 * seq_len)\n",
    "acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training transformer for sequence reversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vocab = 11\n",
    "seq_len = 10\n",
    "model_dim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_data_generator(num_batches=30, batch_size=20):\n",
    "    for _ in range(num_batches):\n",
    "        data = np.random.randint(num_vocab, size=(batch_size, seq_len))\n",
    "        src = torch.from_numpy(data)\n",
    "        tgt = torch.from_numpy(data[:, ::-1].copy())\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(input_vocab=num_vocab, output_vocab=num_vocab, model_dim=model_dim, num_coder=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "noam_optimizer = NoamOptimizer(adam_optimizer, model_dim, 1, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.3041552821795146\n",
      "1 1.8227254152297974\n",
      "2 1.6300251364707947\n",
      "3 1.307507570584615\n",
      "4 0.973494819800059\n",
      "5 0.8183308084805806\n",
      "6 0.7434642116228739\n",
      "7 0.7893629471460978\n",
      "8 0.7124856233596801\n",
      "9 0.7884847939014434\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    transformer.train()\n",
    "    data_iter = reverse_data_generator()\n",
    "\n",
    "    losses = 0.\n",
    "    cnt = 0\n",
    "\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = transformer(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "\n",
    "        out = out.reshape(-1, out.shape[-1])\n",
    "        labels = batch.trg_y.reshape(-1)\n",
    "\n",
    "        loss = loss_fn(out, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        noam_optimizer.step()\n",
    "        noam_optimizer.optimizer.zero_grad()\n",
    "\n",
    "        losses += loss.item()\n",
    "        cnt += 1\n",
    "\n",
    "    print(epoch, losses / cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing transformer for sequence reversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.randint(num_vocab, size=(10, seq_len))\n",
    "\n",
    "expected_output = torch.from_numpy(input[:, ::-1].copy()) \n",
    "output_init = torch.from_numpy(input[:,-1].reshape(-1,1))\n",
    "input = torch.from_numpy(input)\n",
    "input_mask = torch.ones(10, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer.greedy_decode(input, output_init, seq_len, input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  6, 10,  1,  5,  6,  5,  2,  5, 10],\n",
       "        [ 0, 10,  0,  2,  8, 10,  3,  5,  4,  3],\n",
       "        [10,  5,  8,  8, 10, 10,  0, 10,  5,  8],\n",
       "        [10,  1,  4,  8,  1,  5,  8,  5,  4,  5],\n",
       "        [10,  2,  4,  9,  7,  0,  3,  9,  9,  8],\n",
       "        [10, 10,  3,  2,  9,  2,  1, 10,  5,  2],\n",
       "        [ 7,  4,  4,  1,  4,  1,  0,  1,  6,  9],\n",
       "        [ 4,  8,  6,  1,  8,  0,  3,  2,  0,  1],\n",
       "        [ 7,  3,  1,  4,  8,  4,  9, 10,  5,  8],\n",
       "        [ 7,  5,  8,  3,  4, 10,  9,  7,  4,  4]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  6, 10,  1,  5,  6,  5,  2,  5, 10],\n",
       "        [ 0, 10,  5, 10,  2,  8,  5,  5,  4,  3],\n",
       "        [10,  5,  8,  8, 10,  5, 10,  5, 10,  8],\n",
       "        [10,  4,  1,  8,  1,  5,  8,  5,  4,  5],\n",
       "        [10,  2,  4,  9,  7,  5,  3,  9,  9,  9],\n",
       "        [10, 10,  3,  2,  9,  2,  1, 10,  5,  2],\n",
       "        [ 7,  4,  7,  1,  4,  1,  1,  1,  6,  6],\n",
       "        [ 4,  8,  6,  1,  8,  5,  3,  2,  1,  1],\n",
       "        [ 7,  3,  1,  4,  8,  4, 10,  5,  7,  8],\n",
       "        [ 7,  5,  7,  3,  4, 10,  9,  7,  4,  4]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7799999713897705"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get accuracy \n",
    "acc = (output == expected_output).sum() / (10 * seq_len)\n",
    "acc.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
